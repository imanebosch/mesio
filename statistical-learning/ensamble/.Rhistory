library(rsample)
data(srbct)
data <- as.data.frame(srbct$gene)
data["class"] <- srbct$class
# Adaboost
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
#tunegrid <- expand.grid(mfinal = 1000, maxdepth = c(1, 4, 8, 16), coeflearn = "Breiman")
tunegrid <- expand.grid(mfinal = 1000, coeflearn = "Breiman")
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
trControl = ctrl.ada,
tuneGrid = tunegrid,
verbose = TRUE)
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
trControl = ctrl.ada,
mfinal = 1000,
verbose = TRUE)
# Adaboost
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
#tunegrid <- expand.grid(mfinal = 1000, maxdepth = c(1, 4, 8, 16), coeflearn = "Breiman")
tunegrid <- expand.grid(mfinal = 1000, maxdepth = c(1, 4, 8, 16), coeflearn = "Breiman")
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
mfinal = 1000,
verbose = TRUE)
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
mfinal = 1000,
verbose = TRUE)
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
mfinal = 1000,
verbose = TRUE)
?train
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy"
mfinal = 1000,
verbose = TRUE)
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy",
mfinal = 1000,
verbose = TRUE)
warning()
warnings()
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy",
mfinal = 1000,
coeflearn = "Breiman",
maxdepth=1)
# Adaboost
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
tunegrid <- expand.grid(mfinal = 40, maxdepth = c(1), coeflearn = "Breiman")
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
# Adaboost
#ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
ctrl.ada <- trainControl(method = "cv", number = 3)
tunegrid <- expand.grid(mfinal = 40, maxdepth = c(1), coeflearn = "Breiman")
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
ada
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
#ctrl.ada <- trainControl(method = "cv", number = 3)
tunegrid <- expand.grid(mfinal = 40, maxdepth = c(1), coeflearn = "Breiman")
#ctrl.ada <- trainControl(method = "cv", number = 3)
tunegrid <- expand.grid(mfinal = 1000, maxdepth = c(1), coeflearn = "Breiman")
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(gplots)
library(rpart.plot)
library(randomForest)
library(mixOmics)
library(dplyr)
library(adabag)
library(ggplot2)
library(tidyr)
library(xtable)
library(rsample)
data(srbct)
data <- as.data.frame(srbct$gene)
data["class"] <- srbct$class
#method customization
customRF <- list(type = "Classification",
library = "randomForest",
loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
class = rep("numeric", 2),
label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
# train model
ctrl.rf <- trainControl(method = "LOOCV")
tunegrid <- expand.grid(.mtry=c(2,10,50), .ntree=c(5,25,100,400))
set.seed(1234)
random.forest <- train(class~.,
data=data,
method=customRF,
metric="Accuracy",
trControl=ctrl.rf,
tuneGrid = tunegrid)
install.packages("xgboost")
library(xgboost)
library(ada)
set.seed(1234)
top30.output <- list()
all.output <- list()
resamples <- bootstraps(data, times = 100)
for (i in 1:length(resamples$splits)){
rf <- randomForest(class~., data = resamples$splits[[i]],
mtry = 50, ntree = 400)
impvar <- importance(rf) # get importance of each variable
all.output[[resamples$id[i]]] <- as.vector(impvar) # save importance for all variables
impvar <- sort(impvar, decreasing = TRUE, index.return = TRUE) #sort variables by importance
names(impvar$x) <- paste("g", impvar$ix, sep = "")
set <- data.frame(impvar$x[1:30])
set$names <- rownames(set)
top30.output[[resamples$id[i]]] <- set[,c(2,1)] # save 30 most important variables
}
join <- data.frame(top30.output[[1]]) %>% rename(b1 = impvar.x.1.30.)
for (i in 2:length(top30.output)){
a <- paste("b",i,sep = "")
join <- full_join(join,top30.output[[i]],by="names") %>% rename(!!a := impvar.x.1.30.)
}
join[is.na(join)] <- 0
join <- join %>% mutate(Mean.Imp = rowMeans(as.data.frame(join[,-1]))) %>%
select(names, Mean.Imp) %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>% rename(Variable := names)
xtable(join)
join %>%
ggplot()+
geom_point(aes(x = factor(Variable, levels = Variable[order(Mean.Imp)]), y = Mean.Imp))+
coord_flip()+
theme_bw()
join %>%
ggplot()+
geom_point(aes(x = factor(Variable, levels = Variable[order(Mean.Imp)]), y = Mean.Imp))+
coord_flip()+
ggtitle(label="Top 30 most important variables")+
xlab("Mean Importance") + ylab("Variable")+
theme_bw()
join %>%
ggplot()+
geom_point(aes(x = factor(Variable, levels = Variable[order(Mean.Imp)]), y = Mean.Imp))+
coord_flip()+
ggtitle(label="Top 30 most important variables")+
xlab("Variable") + ylab("Mean Importance")+
theme_bw()
impor <- data.frame("Mean.Imp" = rowMeans(as.data.frame(all.output)))
impor["Variable"] <- paste("g", 1:(ncol(data)-1), sep = "")
impor <- impor %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>% select(Variable,Mean.Imp)
xtable(impor)
xtable(impor)
impor %>%
ggplot()+
geom_point(aes(x = factor(Variable, levels = Variable[order(Mean.Imp)]), y = Mean.Imp))+
coord_flip()+
ggtitle(label="Top 30 most important variables")+
xlab("Variable") + ylab("Mean Importance")+
theme_bw()
library(gbm)
?gbm
gbm(class~.,data=data,distribution="adaboost",n.trees=10,
interaction.depth=1,shrinkage=0.1,verbose=F)
?adaboost.M1
?rpart.control
fil <- data.frame("Mean.Imp" = rowMeans(as.data.frame(all.output)))
fil["Variable"] <- paste("g", 1:(ncol(data)-1), sep = "")
fil30 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>% select(Variable,Mean.Imp)
fil50 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(50, Mean.Imp) %>% select(Variable,Mean.Imp)
fil100 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(100, Mean.Imp) %>% select(Variable,Mean.Imp)
head(fil30)
fil30[1,]
head(fil30[,1])
head(data[,fil30[,1]])
data30 <- data[,fil30[,1]]
data50 <- data[,fil50[,1]]
data100 <- data[,fil100[,1]]
# Adaboost
#ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
ctrl.ada <- trainControl(method = "cv", number = 10)
tunegrid <- expand.grid(mfinal = 100, maxdepth = c(1), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
class(data)
data30 <- data[,c(fil30[,1],"class")]
data30 <- data[,c(fil30[,1],"class")]
data50 <- data[,c(fil50[,1],"class")]
data100 <- data[,c(fil100[,1],"class")]
# Adaboost
#ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
ctrl.ada <- trainControl(method = "cv", number = 10)
tunegrid <- expand.grid(mfinal = 100, maxdepth = c(1), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
ada
?train
# Adaboost
#ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
tunegrid <- expand.grid(mfinal = 100, maxdepth = c(4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
ctrl.ada <- trainControl(method = "LOOCV", search = "grid")
tunegrid <- expand.grid(mfinal = 100, maxdepth = c(1,4), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
boosting.cv(class ~ ., v = 63, data = data30, mfinal = 100, control = rpart.control(maxdepth = 1))
?train
boosting.cv(class ~ ., v = 10, data = data30, mfinal = 100, control = rpart.control(maxdepth = 4))
# Adaboost
ctrl.ada <- trainControl(method = "cv", k=10, search = "grid")
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=10, search = "grid")
tunegrid <- expand.grid(mfinal = 100, maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(gplots)
library(rpart.plot)
library(randomForest)
library(mixOmics)
library(dplyr)
library(adabag)
library(ggplot2)
library(tidyr)
library(xtable)
library(rsample)
data(srbct)
data <- as.data.frame(srbct$gene)
data$class <- srbct$class
#method customization
customRF <- list(type = "Classification",
library = "randomForest",
loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
class = rep("numeric", 2),
label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
# train model
ctrl.rf <- trainControl(method = "LOOCV")
tunegrid <- expand.grid(.mtry=c(2,10,50), .ntree=c(5,25,100,400))
set.seed(1234)
random.forest <- train(class~.,
data=data,
method=customRF,
metric="Accuracy",
trControl=ctrl.rf,
tuneGrid = tunegrid)
install.packages(doParallel)
library(doParallel)
cl <- makePSOCKcluster(3)
cl
registerDoParallel(cl)
sum <- spread(random.forest$results[,1:3], ntree, Accuracy, fill = NA, convert = FALSE)[,-1]
rownames(sum) <- c("2", "10", "50")
print(xtable(sum, caption = "Accuracy rates for each possible parameter combination"))
res <- as.matrix(sum)
heatmap.2(res, dendrogram = "none", trace = "none", density.info = "none",
Rowv = FALSE, rowsep = c(1,2), colsep = c(1,2,3,4),
xlab = "parameter ntree", ylab = "parameter mtry", main = "Accuracy heatmap",
cexRow=0.9, cexCol=0.9)
set.seed(1234)
top30.output <- list()
all.output <- list()
resamples <- bootstraps(data, times = 100)
for (i in 1:length(resamples$splits)){
rf <- randomForest(class~., data = resamples$splits[[i]],
mtry = 50, ntree = 400)
impvar <- importance(rf) # get importance of each variable
all.output[[resamples$id[i]]] <- as.vector(impvar) # save importance for all variables
impvar <- sort(impvar, decreasing = TRUE, index.return = TRUE) #sort variables by importance
names(impvar$x) <- paste("g", impvar$ix, sep = "")
set <- data.frame(impvar$x[1:30])
set$names <- rownames(set)
top30.output[[resamples$id[i]]] <- set[,c(2,1)] # save 30 most important variables
}
splitted <- strsplit(system("ps -C rsession -o %cpu,%mem,pid,cmd", intern = TRUE), " ")
df <- do.call(rbind, lapply(splitted[-1],
function(x) data.frame(
cpu = as.numeric(x[2]),
mem = as.numeric(x[4]),
pid = as.numeric(x[5]),
cmd = paste(x[-c(1:5)], collapse = " "))))
df
join <- data.frame(top30.output[[1]]) %>% rename(b1 = impvar.x.1.30.)
for (i in 2:length(top30.output)){
a <- paste("b",i,sep = "")
join <- full_join(join,top30.output[[i]],by="names") %>% rename(!!a := impvar.x.1.30.)
}
join[is.na(join)] <- 0
join <- join %>% mutate(Mean.Imp = rowMeans(as.data.frame(join[,-1]))) %>%
select(names, Mean.Imp) %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>% rename(Variable := names)
print(xtable(join, caption = "Top 30 most important variables method 1"))
join %>%
ggplot()+
geom_point(aes(x = factor(Variable, levels = Variable[order(Mean.Imp)]), y = Mean.Imp))+
coord_flip()+
ggtitle(label="Top 30 most important variables")+
xlab("Variable") + ylab("Mean Importance")+
theme_bw()
impor <- data.frame("Mean.Imp" = rowMeans(as.data.frame(all.output)))
impor["Variable"] <- paste("g", 1:(ncol(data)-1), sep = "")
impor <- impor %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>% select(Variable,Mean.Imp)
print(xtable(impor, caption = "Top 30 most important variables method 2"))
impor %>%
ggplot()+
geom_point(aes(x = factor(Variable, levels = Variable[order(Mean.Imp)]), y = Mean.Imp))+
coord_flip()+
ggtitle(label="Top 30 most important variables")+
xlab("Variable") + ylab("Mean Importance")+
theme_bw()
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = 50, maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
fil <- data.frame("Mean.Imp" = rowMeans(as.data.frame(all.output)))
fil["Variable"] <- paste("g", 1:(ncol(data)-1), sep = "")
fil30 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>% select(Variable,Mean.Imp)
fil50 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(50, Mean.Imp) %>% select(Variable,Mean.Imp)
fil100 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(100, Mean.Imp) %>% select(Variable,Mean.Imp)
data30 <- data[,c(fil30[,1],"class")]
data50 <- data[,c(fil50[,1],"class")]
data100 <- data[,c(fil100[,1],"class")]
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = 50, maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
ada
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = c(5,10,,25,50,75), maxdepth = c(1,4,8,16), coeflearn = "Breiman")
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = c(5,10,25,50,75), maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
ada
plot(ada)
plot(ada)
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = c(1,5,10,25,50,75,100), maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data100,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
plot(ada)
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=10, search = "grid")
tunegrid <- expand.grid(mfinal = c(1,5,10,25,50,75,100), maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=10, search = "grid")
tunegrid <- expand.grid(mfinal = c(1,5,10,25,50,75,100), maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=10, search = "grid")
tunegrid <- expand.grid(mfinal = c(5,10,25,50,75,100), maxdepth = c(1,4,8,16), coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
sum <- spread(random.forest$results[,1:3], ntree, Accuracy, fill = NA, convert = FALSE)[,-1]
rownames(sum) <- c("2", "10", "50")
print(xtable(sum, caption = "Accuracy rates for each possible parameter combination"))
ada
plot(ada)
ada
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = c(5,10,50,100,500), maxdepth = c(1,4,8,16),
coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = c(5,10,50,100,500), maxdepth = c(1,4,8,16),
coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
fil <- data.frame("Mean.Imp" = rowMeans(as.data.frame(all.output)))
fil["Variable"] <- paste("g", 1:(ncol(data)-1), sep = "")
fil30 <- fil %>% arrange(desc(Mean.Imp)) %>% top_n(30, Mean.Imp) %>%
select(Variable,Mean.Imp)
data30 <- data[,c(fil30[,1],"class")]
# Adaboost
ctrl.ada <- trainControl(method = "cv", number=5, search = "grid")
tunegrid <- expand.grid(mfinal = c(5,10,50,100,500), maxdepth = c(1,4,8,16),
coeflearn = "Breiman")
ada <- train(class~.,
data = data30,
method ="AdaBoost.M1",
metric="Accuracy",
trControl=ctrl.ada,
tuneGrid = tunegrid)
ada
plot(ada)
ada
ada
plot(random.forest)
plot(random.forest, main="afaf")
plot(random.forest, main="afaf")
plot(random.forest, main="Performance of the random forest")
plot(ada, cex=0.8)
plot(ada, cex=0.8, main="Performance Adaboost")
plot(ada, cex=0.8, main="Performance Adaboost", main.cex=0.8)
plot(ada, cex=0.8, main="Performance Adaboost Model")
plot(ada, cex=0.8, topleft, main="Performance Adaboost Model")
plot(ada, cex=0.8, main="Performance Adaboost Model")
?adaboost.M1
?plot.errorevol
plot(ada, cex=0.8, main="Performance Adaboost Model")
ada
plot(ada, cex=0.8, main="Performance Adaboost Model")
