preProc= c("center","scale"))
single.tree
prp(single.tree$finalModel,box.palette = "Reds",tweak = 1.2, main = "Single tree classification")
pred <- predict(single.tree, data_test)
confusionMatrix(pred,data_test$y)
single.tree.roc <- roc(as.numeric(data_test$y),as.numeric(pred))
plot(x=single.tree.roc$specificities, y=single.tree.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
single.tree.roc$auc
single.tree.roc <- roc(as.numeric(data_test$y),as.numeric(pred))
plot(y=single.tree.roc$specificities, x=single.tree.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
single.tree.roc$auc
single.tree.roc <- roc(as.numeric(data_test$y),as.numeric(pred))
plot(x=single.tree.roc$specificities, y=single.tree.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
single.tree.roc$auc
single.tree.roc <- roc(as.numeric(data_test$y),as.numeric(pred))
plot(x=single.tree.roc$specificities, y=1-single.tree.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
single.tree.roc$auc
single.tree.roc <- roc(as.numeric(data_test$y),as.numeric(pred))
plot(x=1-single.tree.roc$specificities, y=single.tree.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
single.tree.roc$auc
#method customization
customRF <- list(type = "Classification",
library = "randomForest",
loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
class = rep("numeric", 2),
label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
# train model
ctrl.rf <- trainControl(method = "repeatedcv",number = 5, repeats=1) #reduced due to computational costs (5-fold cross-validation with one repetition)
tunegrid <- expand.grid(.mtry=c(2,5,10), .ntree=c(100, 250,500))
set.seed(1234)
random.forest <- train(y~., data=data_train, method=customRF, metric="Accuracy", trControl=ctrl.rf, preProc= c("center","scale"),tuneGrid = tunegrid)
random.forest
plot(random.forest)
random.forest.final <- randomForest(y~.,data=data_train ,mtry=5, ntree = 100)
random.forest.final
varImpPlot(random.forest.final)
random.forest.pred <- predict(random.forest.final,data_test)
confusionMatrix(random.forest.pred,data_test$y)
random.forest.roc <- roc(as.numeric(data_test$y),as.numeric(random.forest.pred))
plot(x=random.forest.roc$specificities, y=random.forest.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
random.forest.roc$auc
random.forest.roc <- roc(as.numeric(data_test$y),as.numeric(random.forest.pred))
plot(x=1-random.forest.roc$specificities, y=random.forest.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
random.forest.roc$auc
AdaBoost <- gbm(y~.,
data=data_train,
distribution = "adaboost",
n.trees = 2000,
interaction.depth = 1,
cv.folds = 5)
gbm.perf(AdaBoost)
AdaBoost.pred.train <- predict(AdaBoost,newdata=data_train,n.trees=2000,type="response")
confusionMatrix(as.factor(AdaBoost.pred.train),data_train$y)
AdaBoost.pred.test <- predict(AdaBoost,newdata=data_test,n.trees=2000,type="response")
confusionMatrix(as.factor(AdaBoost.pred.test),data_test$y)
AdaBoost.pred.train <- predict(AdaBoost,newdata=data_train,n.trees=2000,type="response")
confusionMatrix(as.factor(AdaBoost.pred.train),data_train$y)
AdaBoost.stump=gbm(y~.,data=data_train,
distribution="adaboost",
n.trees=500,
interaction.depth=1,
cv.folds = 5)
princ <- princomp( as.matrix(data), scale = FALSE )
data <- as.data.frame(read.csv("soldat.csv",header = T,sep = ","))
data$y<-ifelse(data$y==-1,0,1)
data$y<-as.factor(data$y)
p.m <- sum(is.na(data))/(dim.data.frame(data)[1]*dim.data.frame(data)[2])*100
paste(round(p.m,2),"% missing values")
data <- data[,-71]
princ <- princomp( as.matrix(data), scale = FALSE )
data <- data.frame(data)
princ <- princomp( as.matrix(data), scale = FALSE )
data <- scale(data)
data <- scale(data)
data <- as.data.frame(read.csv("soldat.csv",header = T,sep = ","))
data$y<-ifelse(data$y==-1,0,1)
data$y<-as.factor(data$y)
p.m <- sum(is.na(data))/(dim.data.frame(data)[1]*dim.data.frame(data)[2])*100
paste(round(p.m,2),"% missing values")
data <- data[,-71]
data <- scale(data)
data <- data.frame(data)
princ <- princomp( as.matrix(data), scale = FALSE )
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv(file = "soldat.csv")
head(data)
data <- data[ , -71]
data <- scale(data)
data <- data.frame(data)
princ <- princomp( as.matrix(data), scale = FALSE )
summary(princ)
plot(princ)
biplot(princ, main= "Biplot Components 1-2")
data <- as.data.frame(read.csv("soldat.csv",header = T,sep = ","))
data$y<-ifelse(data$y==-1,0,1)
data$y<-as.factor(data$y)
data <- as.data.frame(read.csv("soldat.csv",header = T,sep = ","))
data$y<-ifelse(data$y==-1,0,1)
data$y<-as.factor(data$y)
p.m <- sum(is.na(data))/(dim.data.frame(data)[1]*dim.data.frame(data)[2])*100
paste(round(p.m,2),"% missing values")
data <- data[,-71]
data.pca <- data[,-72]
princ <- princomp( as.matrix(data.pca), scale = FALSE )
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
summary(princ)
plot(princ)
biplot(princ, main= "Biplot Components 1-2")
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
plot(princ)
biplot(princ, main= "Biplot Components 1-2") (no serveix per res aquest biplot amb tanta dada)
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
plot(princ)
biplot(princ, main= "Biplot Components 1-2")
princ
names(princ)
princ$scores
?princomp
plot(princ$scores[,1],princ$scores[,2])
plot(princ$scores[,1],princ$scores[,2], cex=0.4)
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
plot(princ)
plot(princ$scores[,1],princ$scores[,2], cex=0.4, col=data$y)
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
plot(princ)
plot(princ$scores[,1],princ$scores[,2], cex=0.4)
nrow(princ$scores[,1])
length(princ$scores[,1])
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
plot(princ)
plot(princ$scores[,1],princ$scores[,2], cex=0.4)
screeplot(princ, type = "lines")
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.4)
biplot(princ)
biplot(princ, choices = c(1,3), main= "Biplot Components 1-3")
biplot(princ, choices = c(2,3), main= "Biplot Components 2-3")
?biplot
data.pca <- scale(data[,-72], scale = FALSE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
biplot(princ, choices = c(1,2), main= "Biplot Components 1-2")
plot(princ$scores[,1],princ$scores[,2], cex=0.4)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.4)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.4, col=data$y)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.4, col=data$y, sp=1)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.4, col=data$y, asp=1)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.3, col=data$y, asp=1)
plot(princ$scores[,1],princ$scores[,3], cex=0.3, col=data$y, asp=1)
plot(princ$scores[,2],princ$scores[,3], cex=0.3, col=data$y, asp=1)
# proportion of variance explained by each PC
princ$sd^2/sum(princ$sd^2)
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2)
# proportion of variance explained by each PC
princ$sd^2/sum(princ$sd^2)
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2)
s <- cov(data)
load.soldat <- princ$loadings
par(mar = c(1,4,0,2), mfrow = c(6,1))
load.soldat[1:7,]
load=load.soldat[abs(load.soldat)>1e-2]
for(i in 1:6) barplot(load.soldat[,i], ylim = c(-1, 1))
princ2 <- princomp( as.matrix(data), scale = TRUE )
princ2 <- princomp( as.matrix(data.pca),)
summary(princ2)
biplot(princ)
biplot(princ, choices = c(1,3), main= "Biplot Components 1-3")
biplot(princ, choices = c(2,3), main= "Biplot Components 2-3")
load.soldat <- princ$loadings
par(mar = c(1,4,0,2), mfrow = c(6,1))
load.soldat[1:7,]
load=load.soldat[abs(load.soldat)>1e-2]
for(i in 1:6) barplot(load.soldat[,i], ylim = c(-1, 1))
knitr::opts_chunk$set(echo = TRUE)
Dm <- as.matrix(dist(data[1:500,-72], method = "manhattan"))
data <- as.data.frame(read.csv("soldat.csv",header = T,sep = ","))
data$y<-ifelse(data$y==-1,0,1)
data$y<-as.factor(data$y)
p.m <- sum(is.na(data))/(dim.data.frame(data)[1]*dim.data.frame(data)[2])*100
paste(round(p.m,2),"% missing values")
data <- data[,-71]
Dm <- as.matrix(dist(data[1:500,-72], method = "manhattan"))
out.mds <- cmdscale(Dm, eig = TRUE, k = 2)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.3, col=data$y, asp=1)
Dm <- as.matrix(dist(data[1:500,-72], method = "manhattan"))
out.mds <- cmdscale(Dm, eig = TRUE, k = 2)
Dm <- as.matrix(dist(data[1:500,-72], method = "manhattan"))
out.mds <- cmdscale(Dm, eig = TRUE, k = 2)
par(mgp=c(2,0.5,0))
plot(out.mds$points[,1:2],  asp = 1,
cex.axis=0.8,cex.main=0.9, xlab = "Primera dimensión",
ylab = "Segunda dimensión", main = "Aproximación bi-dimensional")
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.3, col=data$y, asp=1)
?sample
sample(1:nrow(data),500)
sample <- sample(1:nrow(data),500)
Dm <- as.matrix(dist(data[sample,-72], method = "manhattan"))
out.mds <- cmdscale(Dm, eig = TRUE, k = 2)
par(mgp=c(2,0.5,0))
plot(out.mds$points[,1:2],  asp = 1,
cex.axis=0.8,cex.main=0.9, xlab = "Primera dimensión",
ylab = "Segunda dimensión", main = "Aproximación bi-dimensional")
plot(out.mds$points[,1:2],  asp = 1, col=data[sample,72],
cex.axis=0.8,cex.main=0.9, xlab = "Primera dimensión",
ylab = "Segunda dimensión", main = "Aproximación bi-dimensional")
sample <- sample(1:nrow(data),500)
Dm <- as.matrix(dist(data[sample,-72], method = "manhattan"))
out.mds <- cmdscale(Dm, eig = TRUE, k = 2)
par(mgp=c(2,0.5,0))
plot(out.mds$points[,1:2],  asp = 1, col=data[sample,72],
cex.axis=0.8,cex.main=0.9, xlab = "Primera dimensión",
ylab = "Segunda dimensión", main = "Aproximación bi-dimensional")
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
screeplot(princ, type = "lines")
plot(princ$scores[,1],princ$scores[,2], cex=0.3, col=data$y, asp=1)
screeplot(princ, type = "lines")
plot(princ)
sample <- sample(1:nrow(data),500)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
plot(princ)
princ
princ$sd^2/sum(princ$sd^2)
princ$sd[1]^2
# cumulative proportion of explained variance
cumsum(princ$sd[1]^2+princ$sd[2]^2)/sum(princ$sd^2)
# cumulative proportion of explained variance
cumsum(princ$sd[1]^2+princ$sd[2]^2+princ$sd[3]^2)/sum(princ$sd^2)
# proportion of variance explained by each PC
var <- princ$sd^2/sum(princ$sd^2)
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2)
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2)[7]
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2)
# cumulative proportion of explained variance
(cumsum(princ$sd^2)/sum(princ$sd^2))[7]
# cumulative proportion of explained variance
(cumsum(princ$sd^2)/sum(princ$sd^2))[6]
princ$sd^2/sum(princ$sd^2)
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2))
# cumulative proportion of explained variance
cumsum(princ$sd^2)/sum(princ$sd^2)
# cumulative proportion of explained variance
plot(cumsum(princ$sd^2)/sum(princ$sd^2))
# cumulative proportion of explained variance
plot(1-(cumsum(princ$sd^2)/sum(princ$sd^2)))
# cumulative proportion of explained variance
plot(cumsum(princ$sd^2)/sum(princ$sd^2)))
# cumulative proportion of explained variance
plot(cumsum(princ$sd^2)/sum(princ$sd^2))
abline(h=0.8)
plot(cumsum(princ$sd^2)/sum(princ$sd^2))
abline(h=0.8)
abline(h=0.8, lty=2, col="red")
plot(cumsum(princ$sd^2)/sum(princ$sd^2))
abline(h=0.8, lty=2, col="red")
cumsum(princ$sd^2)/sum(princ$sd^2)
abline(h=0.8, lty=2, col="red")
plot(cumsum(princ$sd^2)/sum(princ$sd^2))
abline(h=0.8, lty=2, col="red")
plot(cumsum(princ$sd^2)/sum(princ$sd^2), xlab = "N Components" , ylab = "% Explained Variance")
abline(h=0.8, lty=2, col="red")
plot(princ)
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
plot(cumsum(princ$sd^2)/sum(princ$sd^2), xlab = "N Components" , ylab = "% Explained Variance")
par(margin=c(3,5,2,5))
plot(cumsum(princ$sd^2)/sum(princ$sd^2), xlab = "N Components" , ylab = "% Explained Variance")
plot(cumsum(princ$sd^2)/sum(princ$sd^2), xlab = "N Components" , ylab = "% Explained Variance")
abline(h=0.8, lty=2, col="red")
plot(princ$scores[,1],princ$scores[,2], cex=0.3,
col=data$y, asp=1, xlab = "Primer COmponente", ylab = "Segundo Componen")
load.soldat <- princ$loadings
par(mar = c(1,4,0,2), mfrow = c(6,1))
load.soldat[1:7,]
load=load.soldat[abs(load.soldat)>1e-2]
for(i in 1:6) barplot(load.soldat[,i], ylim = c(-1, 1))
load.soldat <- princ$loadings
par(mar = c(1,4,0,2), mfrow = c(6,1))
load=load.soldat[abs(load.soldat)>1e-2]
for(i in 1:6) barplot(load.soldat[,i], ylim = c(-1, 1))
set.seed(1234)
pt <- 1/2
index <- createDataPartition(y = data$y ,p = pt,list = FALSE) #data partition
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(rpart.plot)
library(randomForest)
library(pROC)
library(ROC632)
library(gbm)
library(ISLR)
data <- as.data.frame(read.csv("soldat.csv",header = T,sep = ","))
data$y<-ifelse(data$y==-1,0,1)
data$y<-as.factor(data$y)
p.m <- sum(is.na(data))/(dim.data.frame(data)[1]*dim.data.frame(data)[2])*100
paste(round(p.m,2),"% missing values")
data <- data[,-71]
data.pca <- scale(data[,-72], scale = TRUE)
princ <- princomp( as.matrix(data.pca))
plot(cumsum(princ$sd^2)/sum(princ$sd^2), xlab = "N Components" , ylab = "% Explained Variance")
abline(h=0.8, lty=2, col="red")
plot(princ$scores[,1],princ$scores[,2], cex=0.3,
col=data$y, asp=1, xlab = "Primer Componente", ylab = "Segundo Componente")
set.seed(1234)
pt <- 1/2
index <- createDataPartition(y = data$y ,p = pt,list = FALSE) #data partition
data_train <- data[-index,] #train set
data_test <- data[index,] #test set
ctrl <- trainControl(method = "repeatedcv",number = 10, repeats=3)
single.tree <- train(y ~ .,
data = data_train,
method="rpart",
trControl=ctrl,
metric="Accuracy",
preProc= c("center","scale"))
single.tree
prp(single.tree$finalModel,box.palette = "Reds",tweak = 1.2, main = "Single tree classification")
pred <- predict(single.tree, data_test)
confusionMatrix(pred,data_test$y)
confusionMatrix(pred,data_test$y)
single.tree.roc <- roc(as.numeric(data_test$y),as.numeric(pred))
plot(x=1-single.tree.roc$specificities, y=single.tree.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
single.tree.roc$auc
#method customization
customRF <- list(type = "Classification",
library = "randomForest",
loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
class = rep("numeric", 2),
label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
# train model
ctrl.rf <- trainControl(method = "repeatedcv",number = 5, repeats=1) #reduced due to computational costs (5-fold cross-validation with one repetition)
tunegrid <- expand.grid(.mtry=c(2,5,10), .ntree=c(100, 250,500))
set.seed(1234)
random.forest <- train(y~., data=data_train, method=customRF, metric="Accuracy", trControl=ctrl.rf, preProc= c("center","scale"),tuneGrid = tunegrid)
random.forest
plot(random.forest)
library(ada)
install.packages("ada")
library(ada)
random.forest.final <- randomForest(y~.,data=data_train ,mtry=5, ntree = 100)
random.forest.final
varImpPlot(random.forest.final)
random.forest.pred <- predict(random.forest.final,data_test)
confusionMatrix(random.forest.pred,data_test$y)
random.forest.roc <- roc(as.numeric(data_test$y),as.numeric(random.forest.pred))
plot(x=1-random.forest.roc$specificities, y=random.forest.roc$sensitivities, ylab="Sensitivity=True Positive Rates", xlab="1-Specificity = False Positive Rates", type="s", lwd=1, main="ROC Curve")
clip(x1=0,x2=1,y1=0,y2=1)
abline(a=0, b=1,col=2,lty=3)
abline(v=0,col=1,lty=3)
abline(h=1,col=1,lty=3)
random.forest.roc$auc
AdaBoost <- ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=200,
rpart.control(maxdepth = 1, cp = -1, minsplit=0))
plot(AdaBoost)
AdaBoost
plot(AdaBoost)
AdaBoost
plot(AdaBoost)
AdaBoost
AdaBoost.pred.train <- predict(AdaBoost,newdata=data_train,n.trees=200)
confusionMatrix(as.factor(AdaBoost.pred.train),data_train$y)
AdaBoost.pred.test <- predict(AdaBoost,newdata=data_test,n.trees=200)
confusionMatrix(as.factor(AdaBoost.pred.test),data_test$y)
AdaBoost.stump=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 1, cp = -1, minsplit=0))
AdaBoost.stump=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 1, cp = -1, minsplit=0))
AdaBoost.4=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 4, cp = -1, minsplit=0))
AdaBoost.8=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 8, cp = -1, minsplit=0))
AdaBoost.16=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 16, cp = -1, minsplit=0))
AdaBoost.pred.test <- predict(AdaBoost,newdata=data_test,n.trees=200)
confusionMatrix(as.factor(AdaBoost.pred.test),data_test$y)
AdaBoost.stump=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 1, cp = -1, minsplit=0))
AdaBoost.4=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 4, cp = -1, minsplit=0))
AdaBoost.8=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 8, cp = -1, minsplit=0))
AdaBoost.16=ada(x=data_train[,-72],
y=data_train[,72],
loss="exponential",
iter=50,
rpart.control(maxdepth = 16, cp = -1, minsplit=0))
plot(AdaBoost.stump)
AdaBoost.stump.test <- predict(AdaBoost.stump,newdata=data_test,n.trees=50)
confusionMatrix(as.factor(AdaBoost.stump.test),data_test$y)
AdaBoost.4.test <- predict(AdaBoost.4,newdata=data_test,n.trees=50)
confusionMatrix(as.factor(AdaBoost.4.test),data_test$y)
AdaBoost.8.test <- predict(AdaBoost.8,newdata=data_test,n.trees=50)
confusionMatrix(as.factor(AdaBoost.8.test),data_test$y)
AdaBoost.16.test <- predict(AdaBoost.16,newdata=data_test,n.trees=50)
confusionMatrix(as.factor(AdaBoost.16.test),data_test$y)
