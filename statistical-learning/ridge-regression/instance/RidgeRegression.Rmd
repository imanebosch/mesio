---
title: "Ridge Regression Assignment"
author: "Ignasi Mañé, Antoni Company & Chiara Barbi"
date: "19/02/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, include=FALSE}

##################################
##R FUNCTIONS FOR THE ASSIGNMENT##
##################################

#Code with the definition of the functions used throughout the document
#this piece of code won't be shown in the pdf document

plot.lam <- function(results,lambda,df) { 
#function that plots the mspe agains both lambda and df(lambda)  

par(mfrow=c(1,2),oma = c(2,3,1,0), mar = c(4,2,2,1), mgp=c(1.5,0.5,0))

plot(log(1+results[,1]),results[,2],
     xlab="log(1+lambda)",
     ylab = "",
     main="MSPE by lambda values", 
     cex.main=0.9, cex.axis=0.8)
lam.opt <- results[results[,2]==min(results[,2]),1]
abline(v=log(1+lam.opt),col=2,lty=2) 

plot(df,results[,2],
     xlab="df(lambda)",
     ylab = "",
     main="MSPE by df(lambda)", 
     cex.main=0.9,cex.axis=0.8)
lam.opt <- df[results[,2]==min(results[,2])]
abline(v=lam.opt,col=2,lty=2)

mtext(text="MSPE",side=2,line=0,outer=TRUE)
}

cal.edf <- function(x,lambda) {
#function that computes the efective degrees of freedom for a
#given vecotor of lambdas and a data set x
  
  n.lam <- length(lambda)
  d2 <- eigen(t(x)%*%x ,symmetric = TRUE, only.values = TRUE)$values
  p <- dim(x)[2]
  n <- dim(x)[1]
  
  df.v <- NULL
  for (i in 1:n.lam){
      lambda.v <- lambda[i]
      df.v[i] <- sum(d2/(d2+lambda.v))
  }  
  
  return(df.v)
}

beta_hat <- function(x,y,lambda) {
#function that computes for a given data and lambda the value
#of the vecotr of regressors and the matrices H and Hlam
  
  p <- dim(x)[2]
  H.lam <- solve(t(x)%*%x + lambda*diag(1,p))%*%t(x)
  beta.hat <- H.lam%*%y
  Hhat <- x%*%H.lam
  
  values <- list(H=H.lam,beta=beta.hat,Hlam=Hhat)
  
}

fit.ridge <- function(x,y,xval,yval,lambda) {
#function that computes for a given data and lambda
#the model that best fits the data and returns the 
#mpse and the vector of betas
  
  p <- dim(x)[2]
  
  reg <- beta_hat(x,y,lambda)
  y.hat <- xval%*%reg$beta
  mspe <- (1/length(yval))*t((yval-y.hat))%*%(yval-y.hat)

  values <- list(beta=reg$beta,mspe=mspe) 
  
  return(values)

}

loo.g.cv <- function(x,y,lambda,type="loocv") {
#function that computes for a given data set and vector 
#of lambdas the value of the E(mspe) computed by gcv o loocv 
   
  n.lambdas <- length(lambda)
  df.v <- cal.edf(x,lambda=lambda)
  n <- dim(x)[1]
  
  results <- matrix(0,nrow = n.lambdas, ncol = 2)
  
  for (l in 1:n.lambdas){ 
    reg <- beta_hat(x,y,lambda[l])
    hat.Y <- x%*%reg$beta
    nu <- sum(diag(reg$Hlam))
    if (type=="gcv") {a<-nu/n} else {a<-diag(reg$Hlam)}
    results[l,] <- c(lambda.v[l],sum(((y-hat.Y)/(1-a))^2 )/n)
  }
  
  plot.lam(results=results,lambda=lambda.v,df=df.v)
  
  opt <- c(results[which.min(results[,2]),],df.v[which.min(results[,2])])
  
  return(opt)
}

```

\section{Introduction}

Write a report that contains the results of the computations that you are asked to carry out below, as well as the explanation of what you are doing. The main text (2 or 3 pages) should include pieces of source code and graphical and numerical output. 

Fist, we load the Boston data set from the MASS package and the libraries required to do the exercise. We will use the caret package to split the data into k random folds whenever it is needed.$\vspace{0.2cm}$

```{r echo=TRUE, message=FALSE}
library(MASS)
library(caret)
data("Boston")
set.seed(5647) 
#we set a seed to generate pseudo random numbers
```

\section{1. Choosing the penalization parameter $\lambda$}

###1. Write an R function implementing the ridge regression penalization parameter $\lambda$ choice based on the minimization of the mean squared prediction error in a validation set ($MSPE_{val}(\lambda)$).

> **Input: Matrix $X$ and vector $y$ corresponding to the training sample; matrix $X_{val}$ and vector $y_{val}$ corresponding to the validation set; a vector lambda.v of candidate values for $\lambda$.**

> **Output: For each element $\lambda$ in lambda.v, the value of $MSPE_{val}(\lambda)$). Additionally you can plot these values against $\log(1+\lambda)-1$ graphic, or against $df(\lambda)$.**

We will create a function that it is used later on in the document.$\vspace{0.2cm}$

```{r echo=TRUE}

ridgereg <- function(x,y,xval,yval,lambda,plot=TRUE) {
  
  p <- dim(x)[2] #number of parameters
  n <- dim(x)[1] #number of observations
  
  #empty results matrix
  results <- matrix(0,nrow = length(lambda), ncol = 2)
  colnames(results) <- c("lambda","pmse")
  
  for (i in 1:length(lambda)) {
      
      mspe <- fit.ridge(x,y,xval,yval,lambda=lambda[i])$mspe
      results[i,] <- c(lambda[i],mspe) 
    
  }
  
  df <- cal.edf(x=x,lambda=lambda)
  results <- cbind(results,df)
  
  if(plot == TRUE){
    
    plot.lam(results=results,lambda=lambda,df=df)
    
  }
  
    return(results)

}

```

###2. Write an R function implementing the ridge regression penalization parameter $df(\lambda)$ choice based on k-fold cross-validation ($MSPE_{val}(\lambda)$)). Input, output and graphics as before.

```{r echo=TRUE}

ridgereg.cv <- function(x,y,k,lambda,plot=TRUE) {
  
  p <- dim(x)[2]
  n <- dim(x)[1]
  flds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
  kresults <- matrix(0,nrow =length(lambda) , ncol = k)
  results <- matrix(0,nrow = length(lambda), ncol = 2)
  colnames(results) <- c("lambda","pmse")
  
  for (j in 1:length(flds)) {
      
      filt <- unlist(flds[j], use.names=FALSE)
      mpse <- ridgereg(x=x[-filt,],y=y[-filt,], 
                       xval=x[filt,],yval=y[filt,],
                       lambda = lambda, 
                       plot = FALSE)
      kresults[,j] <- mpse[,2]
      
    }

  results[,1] <- lambda
  results[,2] <- rowMeans(kresults)
  
  df <- cal.edf(x=x,lambda=lambda)
  results <- cbind(results,df)
  
  if (plot==TRUE) {
      
      plot.lam(results=results,lambda=lambda,df=df)
      
  }
  
  value <- results[which.min(results[,2]),]
  return(value)
}

```

###3. Consider the prostate date used in class. Use your routines to choose the penalization parameter $\lambda$ by the following criteria: behavior in the validation set (the 30 observations not being in the traineng sample); 5-fold and 10-fold cross-validation. Compare your results with those obtained when using leave-one-out and generalized cross-validation.

We will split the data set into a train set and a test set to calculate lambda in exercise 3.1. However, we will use the entire data set in exercices from 3.2 to 3.4 $\vspace{0.2cm}$

```{r echo=TRUE}

prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)
train.index <- which(prostate$train==TRUE)
test <- prostate[-train.index,]
train <- prostate[train.index,]

#Data for point 3.1
Y <- scale( train$lpsa, center=TRUE, scale=FALSE)
X <- scale( as.matrix(train[,1:8]), center=TRUE, scale=TRUE)
Yval <- scale( test$lpsa, center=TRUE, scale=FALSE)
Xval <- scale( as.matrix(test[,1:8]), center=TRUE, scale=TRUE)

#Data for points 3.2, 3.2, 3.3, 3.4
Xtot <- scale(as.matrix(prostate[,1:8]),center=TRUE , scale = TRUE)
Ytot <- scale(prostate$lpsa, center=TRUE , scale=FALSE)

#We create a matrix to store the optimal lambdas
optim.lambdas <- matrix(0,ncol=4,nrow=5)
colnames(optim.lambdas) <- c("method","lambda","pmse","df")
optim.lambdas <- data.frame(optim.lambdas)

#We generate a sequence of lambdas
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1

```

####3.1. Mean squared prediction error

```{r, echo = TRUE}

results.mspe <- ridgereg(x = X, y = Y, xval = Xval , yval = Yval ,lambda = lambda.v,plot = TRUE)
value <- results.mspe[which.min(results.mspe[,2]),] 
optim.lambdas[1,] <- c("mspe",round(value,4))

```

####3.2. 5-Folds Cross-Validation 

```{r echo=TRUE}
results.cv5 <- ridgereg.cv(x=Xtot,y=Ytot,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[2,] <- c("5 fold cv",round(results.cv5,4)) 
```

####3.3. 10-Folds Cross-Validation

```{r echo=TRUE}

results.cv10 <- ridgereg.cv(x=Xtot,y=Ytot,k=10,lambda=lambda.v,plot=TRUE)
optim.lambdas[3,] <- c("10 fold cv",round(results.cv10,4))

```

####3.3. Leave One Out Cross-Validation

Leave one out cross-validation is the particular case of kfold cross-validation in which k=n. However, since the algorithm that computes the kfold cv is very expensive, we will use a useful theoretical result for linear models that requires much less computational power. $\vspace{0.2cm}$

```{r}
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="loocv") 
optim.lambdas[4,] <- c("loo-cv",round(result.loo,4))
```

####3.4. Generalized Cross-Validation

```{r echo=TRUE}
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="gcv") 
optim.lambdas[5,] <- c("gcv",round(result.loo,4))

```

We obtain the following results:

```{r echo=FALSE}
optim.lambdas
```

For each of the cross-validation methods used to estimate the error as a function of lambda we select the value of lambda that minimizes it as the optimal value $\lambda^*$. 

| Method         | $\lambda^*$  |$df(\lambda^*)$  | $E[MSPE]$   |
| :------------- | :----------: | :-----------:   | -----------:|
| mspe           | 20.21        | 5.19            | 0.50        |
| cv. 5 folds    | 3.10         | 7.51            | 0.52        |
| cv. 10 folds   | 5.55         | 7.18            | 0.50        |
| loo-cv         | 5.55         | 7.18            | 0.52        |
| gcv            | 7.29         | 6.97            | 0.52        | 

\section{2. Ridge regression for the Boston Housing data}

###1. The Boston House-price corrected dataset (available in boston.Rdata) contains the same data (with some corrections) and it also includes the UTM coordinates of the geographical centers of each neighborhood. For the Boston House-price corrected dataset use ridge regression to fit the regression model where the response is MEDV and the explanatory variables are the remaining 13 variables in the previous list.

First we choose three different ridge regression penalization parameters $\lambda^*$ using:  5-fold cross validation method, 10-fold cross-validation method and leave-one-out method.

```{r echo=TRUE}
data(Boston)

Y <- scale( Boston$medv, center=TRUE, scale=FALSE)
X <- scale( as.matrix(Boston[,-c(14)]), center=TRUE, scale=TRUE) 
p <- dim(X)[2]
n <- dim(X)[1]
df<-data.frame(X,Y)

optim.lambda <- matrix(0,ncol=4,nrow=3)
colnames(optim.lambda) <- c("method","lambda","pmse","df")
optim.lambda <- data.frame(optim.lambda)

lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1


```


###1.1. 5-Fold Cross Validation
```{r echo=TRUE}

# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
optim.lambda[1,] <- c("5 fold cv",round(results.cv5,4)) 

```

###1.2. 10-Fold Cross Validation
```{r echo=TRUE}
#k - fold 10 cross-validation
results.cv10 <- ridgereg.cv(x=X,y=Y,k=10,lambda=lambda.v,plot=TRUE)
optim.lambda[2,] <- c("10 fold cv",round(results.cv10,4))
```

###1.3. Leave One Out Cross Validation
```{r echo=TRUE}
#loo
results.loo <- loo.g.cv(x=X,y=Y,lambda=lambda.v,type="loocv") 
optim.lambda[3,] <- c("loo",round(results.loo,4)) 
optim.lambda
```

The method that provides the lowest value of the Mean Square Predicted Error is the 5-fold cross validation. Anyway there is not difference between this method and the Leave-one-out method, both in terms of degrees of freedom and $\lambda^*$ value.

Finally, fitting the Ridge regression model using the optimal $\lambda^*$ resulting from the 5-fold cross validation method provides us with the following results:

#1.4. Beta Estimation for the Selected Lambda (scaled)
```{r echo=TRUE}
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=as.numeric(optim.lambda[1,2]))
model$beta

```


