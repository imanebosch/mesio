for (i in 1:length(lambda)) {
mspe <- fit.ridge(x,y,xval,yval,lambda=lambda[i])$mspe
results[i,] <- c(lambda[i],mspe)
}
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if(plot == TRUE){
plot.lam(results=results,lambda=lambda,df=df)
}
return(results)
}
ridgereg.cv <- function(x,y,k,lambda,plot=TRUE) {
p <- dim(x)[2]
n <- dim(x)[1]
flds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
kresults <- matrix(0,nrow =length(lambda) , ncol = k)
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (j in 1:length(flds)) {
filt <- unlist(flds[j], use.names=FALSE)
mpse <- ridgereg(x=x[-filt,],y=y[-filt,],
xval=x[filt,],yval=y[filt,],
lambda = lambda,
plot = FALSE)
kresults[,j] <- mpse[,2]
}
results[,1] <- lambda
results[,2] <- rowMeans(kresults)
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if (plot==TRUE) {
plot.lam(results=results,lambda=lambda,df=df)
}
value <- results[which.min(results[,2]),]
return(value)
}
prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)
train.index <- which(prostate$train==TRUE)
test <- prostate[-train.index,]
train <- prostate[train.index,]
#Data for point 3.1
Y <- scale( train$lpsa, center=TRUE, scale=FALSE)
X <- scale( as.matrix(train[,1:8]), center=TRUE, scale=TRUE)
Yval <- scale( test$lpsa, center=TRUE, scale=FALSE)
Xval <- scale( as.matrix(test[,1:8]), center=TRUE, scale=TRUE)
#Data for points 3.2, 3.2, 3.3, 3.4
Xtot <- scale(as.matrix(prostate[,1:8]),center=TRUE , scale = TRUE)
Ytot <- scale(prostate$lpsa, center=TRUE , scale=FALSE)
#We create a matrix to store the optimal lambdas
optim.lambdas <- matrix(0,ncol=4,nrow=5)
colnames(optim.lambdas) <- c("method","lambda","pmse","df")
optim.lambdas <- data.frame(optim.lambdas)
#We generate a sequence of lambdas
lambda.max <- 1e5
n.lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
results.mspe <- ridgereg(x = X, y = Y, xval = Xval , yval = Yval ,lambda = lambda.v,plot = TRUE)
value <- results.mspe[which.min(results.mspe[,2]),]
optim.lambdas[1,] <- c("mspe",round(value,4))
results.cv5 <- ridgereg.cv(x=Xtot,y=Ytot,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[2,] <- c("5 fold cv",round(results.cv5,4))
results.cv10 <- ridgereg.cv(x=Xtot,y=Ytot,k=10,lambda=lambda.v,plot=TRUE)
optim.lambdas[3,] <- c("10 fold cv",round(results.cv10,4))
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="loocv")
optim.lambdas[4,] <- c("loo-cv",round(result.loo,4))
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="gcv")
optim.lambdas[5,] <- c("gcv",round(result.loo,4))
optim.lambdas
data(Boston)
Y <- scale( Boston$medv, center=TRUE, scale=FALSE)
X <- scale( as.matrix(Boston[,-c(14)]), center=TRUE, scale=TRUE)
p <- dim(X)[2]
n <- dim(X)[1]
df<-data.frame(X,Y)
optim.lambda <- matrix(0,ncol=4,nrow=3)
colnames(optim.lambda) <- c("method","lambda","pmse","df")
optim.lambda <- data.frame(optim.lambda)
lambda.max <- 1e5
n.lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
opt <- round(results.cv5[results.cv5[,2]==min(results.cv5[,2]),],2)
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
results.cv5 <- ridgereg.cv(x=Xtot,y=Ytot,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[1,] <- c("5 fold cv",round(results.cv5,4))
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[1,] <- c("5 fold cv",round(results.cv5,4))
#k - fold 10 cross-validation
results.cv10 <- ridgereg.cv(x=X,y=Y,k=10,lambda=lambda.v,plot=TRUE)
optim.lambdas[2,] <- c("10 fold cv",round(results.cv10,4))
#loo
results.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="loocv")
optim.lambda[3,] <- c("loo",round(results.loo,2))
optim.lambda
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=optim.lambda[3,2])
optim.lambda[3,2]
optim.lambda
#k - fold 10 cross-validation
results.cv10 <- ridgereg.cv(x=X,y=Y,k=10,lambda=lambda.v,plot=TRUE)
optim.lambdas[2,] <- c("10 fold cv",round(results.cv10,4))
optim.lambda
optim.lambdas[2,] <- c("10 fold cv",round(results.cv10,4))
optim.lambdas
optim.lambdas
data(Boston)
Y <- scale( Boston$medv, center=TRUE, scale=FALSE)
X <- scale( as.matrix(Boston[,-c(14)]), center=TRUE, scale=TRUE)
p <- dim(X)[2]
n <- dim(X)[1]
df<-data.frame(X,Y)
optim.lambda <- matrix(0,ncol=4,nrow=3)
colnames(optim.lambda) <- c("method","lambda","pmse","df")
optim.lambda <- data.frame(optim.lambda)
lambda.max <- 1e5
n.lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[1,] <- c("5 fold cv",round(results.cv5,4))
optim.lambdas
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
optim.lambda[1,] <- c("5 fold cv",round(results.cv5,4))
#k - fold 10 cross-validation
results.cv10 <- ridgereg.cv(x=X,y=Y,k=10,lambda=lambda.v,plot=TRUE)
optim.lambda[2,] <- c("10 fold cv",round(results.cv10,4))
#loo
results.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="loocv")
optim.lambda[3,] <- c("loo",round(results.loo,2))
optim.lambda
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=optim.lambda[3,2])
optim.lambda
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=as.numeric(optim.lambda[3,2]))
model$mspe
model$beta
optim.lambda
#loo
results.loo <- loo.g.cv(x=X,y=Y,lambda=lambda.v,type="loocv")
optim.lambda[3,] <- c("loo",round(results.loo,4))
optim.lambda
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=as.numeric(optim.lambda[3,2]))
model$mspe
model$beta
##################################
##R FUNCTIONS FOR THE ASSIGNMENT##
##################################
#Code with the definition of the functions used throughout the document
#this piece of code won't be shown in the pdf document
plot.lam <- function(results,lambda,df) {
#function that plots the mspe agains both lambda and df(lambda)
par(mfrow=c(1,2),oma = c(2,3,1,0), mar = c(4,2,2,1), mgp=c(1.5,0.5,0))
plot(log(1+results[,1]),results[,2],
xlab="log(1+lambda)",
ylab = "",
main="MSPE by lambda values",
cex.main=0.9, cex.axis=0.8)
lam.opt <- results[results[,2]==min(results[,2]),1]
abline(v=log(1+lam.opt),col=2,lty=2)
plot(df,results[,2],
xlab="df(lambda)",
ylab = "",
main="MSPE by df(lambda)",
cex.main=0.9,cex.axis=0.8)
lam.opt <- df[results[,2]==min(results[,2])]
abline(v=lam.opt,col=2,lty=2)
mtext(text="MSPE",side=2,line=0,outer=TRUE)
}
cal.edf <- function(x,lambda) {
#function that computes the efective degrees of freedom for a
#given vecotor of lambdas and a data set x
n.lam <- length(lambda)
d2 <- eigen(t(x)%*%x ,symmetric = TRUE, only.values = TRUE)$values
p <- dim(x)[2]
n <- dim(x)[1]
df.v <- NULL
for (i in 1:n.lam){
lambda.v <- lambda[i]
df.v[i] <- sum(d2/(d2+lambda.v))
}
return(df.v)
}
beta_hat <- function(x,y,lambda) {
#function that computes for a given data and lambda the value
#of the vecotr of regressors and the matrices H and Hlam
p <- dim(x)[2]
H.lam <- solve(t(x)%*%x + lambda*diag(1,p))%*%t(x)
beta.hat <- H.lam%*%y
Hhat <- x%*%H.lam
values <- list(H=H.lam,beta=beta.hat,Hlam=Hhat)
}
fit.ridge <- function(x,y,xval,yval,lambda) {
#function that computes for a given data and lambda
#the model that best fits the data and returns the
#mpse and the vector of betas
p <- dim(x)[2]
reg <- beta_hat(x,y,lambda)
y.hat <- xval%*%reg$beta
mspe <- (1/length(yval))*t((yval-y.hat))%*%(yval-y.hat)
values <- list(beta=reg$beta,mspe=mspe)
return(values)
}
loo.g.cv <- function(x,y,lambda,type="loocv") {
#function that computes for a given data set and vector
#of lambdas the value of the E(mspe) computed by gcv o loocv
n.lambdas <- length(lambda)
df.v <- cal.edf(x,lambda=lambda)
n <- dim(x)[1]
results <- matrix(0,nrow = n.lambdas, ncol = 2)
for (l in 1:n.lambdas){
reg <- beta_hat(x,y,lambda[l])
hat.Y <- x%*%reg$beta
nu <- sum(diag(reg$Hlam))
if (type=="gvc") {a<-nu/n} else {a<-diag(reg$Hlam)}
results[l,] <- c(lambda.v[l],sum(((y-hat.Y)/(1-a))^2 )/n)
}
plot.lam(results=results,lambda=lambda.v,df=df.v)
opt <- c(results[which.min(results[,2]),],df.v[which.min(results[,2])])
return(opt)
}
library(MASS)
library(caret)
data("Boston")
set.seed(5647)
#we set a seed to generate pseudo random numbers
ridgereg <- function(x,y,xval,yval,lambda,plot=TRUE) {
p <- dim(x)[2] #number of parameters
n <- dim(x)[1] #number of observations
#empty results matrix
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (i in 1:length(lambda)) {
mspe <- fit.ridge(x,y,xval,yval,lambda=lambda[i])$mspe
results[i,] <- c(lambda[i],mspe)
}
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if(plot == TRUE){
plot.lam(results=results,lambda=lambda,df=df)
}
return(results)
}
ridgereg.cv <- function(x,y,k,lambda,plot=TRUE) {
p <- dim(x)[2]
n <- dim(x)[1]
flds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
kresults <- matrix(0,nrow =length(lambda) , ncol = k)
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (j in 1:length(flds)) {
filt <- unlist(flds[j], use.names=FALSE)
mpse <- ridgereg(x=x[-filt,],y=y[-filt,],
xval=x[filt,],yval=y[filt,],
lambda = lambda,
plot = FALSE)
kresults[,j] <- mpse[,2]
}
results[,1] <- lambda
results[,2] <- rowMeans(kresults)
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if (plot==TRUE) {
plot.lam(results=results,lambda=lambda,df=df)
}
value <- results[which.min(results[,2]),]
return(value)
}
prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)
train.index <- which(prostate$train==TRUE)
test <- prostate[-train.index,]
train <- prostate[train.index,]
#Data for point 3.1
Y <- scale( train$lpsa, center=TRUE, scale=FALSE)
X <- scale( as.matrix(train[,1:8]), center=TRUE, scale=TRUE)
Yval <- scale( test$lpsa, center=TRUE, scale=FALSE)
Xval <- scale( as.matrix(test[,1:8]), center=TRUE, scale=TRUE)
#Data for points 3.2, 3.2, 3.3, 3.4
Xtot <- scale(as.matrix(prostate[,1:8]),center=TRUE , scale = TRUE)
Ytot <- scale(prostate$lpsa, center=TRUE , scale=FALSE)
#We create a matrix to store the optimal lambdas
optim.lambdas <- matrix(0,ncol=4,nrow=5)
colnames(optim.lambdas) <- c("method","lambda","pmse","df")
optim.lambdas <- data.frame(optim.lambdas)
#We generate a sequence of lambdas
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
results.mspe <- ridgereg(x = X, y = Y, xval = Xval , yval = Yval ,lambda = lambda.v,plot = TRUE)
value <- results.mspe[which.min(results.mspe[,2]),]
optim.lambdas[1,] <- c("mspe",round(value,4))
results.cv5 <- ridgereg.cv(x=Xtot,y=Ytot,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[2,] <- c("5 fold cv",round(results.cv5,4))
results.cv10 <- ridgereg.cv(x=Xtot,y=Ytot,k=10,lambda=lambda.v,plot=TRUE)
optim.lambdas[3,] <- c("10 fold cv",round(results.cv10,4))
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="loocv")
optim.lambdas[4,] <- c("loo-cv",round(result.loo,4))
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="gcv")
optim.lambdas[5,] <- c("gcv",round(result.loo,4))
optim.lambdas
data(Boston)
Y <- scale( Boston$medv, center=TRUE, scale=FALSE)
X <- scale( as.matrix(Boston[,-c(14)]), center=TRUE, scale=TRUE)
p <- dim(X)[2]
n <- dim(X)[1]
df<-data.frame(X,Y)
optim.lambda <- matrix(0,ncol=4,nrow=3)
colnames(optim.lambda) <- c("method","lambda","pmse","df")
optim.lambda <- data.frame(optim.lambda)
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
optim.lambda[1,] <- c("5 fold cv",round(results.cv5,4))
#k - fold 10 cross-validation
results.cv10 <- ridgereg.cv(x=X,y=Y,k=10,lambda=lambda.v,plot=TRUE)
optim.lambda[2,] <- c("10 fold cv",round(results.cv10,4))
#loo
results.loo <- loo.g.cv(x=X,y=Y,lambda=lambda.v,type="loocv")
optim.lambda[3,] <- c("loo",round(results.loo,4))
optim.lambda
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=as.numeric(optim.lambda[3,2]))
model$mspe
model$beta
##################################
##R FUNCTIONS FOR THE ASSIGNMENT##
##################################
#Code with the definition of the functions used throughout the document
#this piece of code won't be shown in the pdf document
plot.lam <- function(results,lambda,df) {
#function that plots the mspe agains both lambda and df(lambda)
par(mfrow=c(1,2),oma = c(2,3,1,0), mar = c(4,2,2,1), mgp=c(1.5,0.5,0))
plot(log(1+results[,1]),results[,2],
xlab="log(1+lambda)",
ylab = "",
main="MSPE by lambda values",
cex.main=0.9, cex.axis=0.8)
lam.opt <- results[results[,2]==min(results[,2]),1]
abline(v=log(1+lam.opt),col=2,lty=2)
plot(df,results[,2],
xlab="df(lambda)",
ylab = "",
main="MSPE by df(lambda)",
cex.main=0.9,cex.axis=0.8)
lam.opt <- df[results[,2]==min(results[,2])]
abline(v=lam.opt,col=2,lty=2)
mtext(text="MSPE",side=2,line=0,outer=TRUE)
}
cal.edf <- function(x,lambda) {
#function that computes the efective degrees of freedom for a
#given vecotor of lambdas and a data set x
n.lam <- length(lambda)
d2 <- eigen(t(x)%*%x ,symmetric = TRUE, only.values = TRUE)$values
p <- dim(x)[2]
n <- dim(x)[1]
df.v <- NULL
for (i in 1:n.lam){
lambda.v <- lambda[i]
df.v[i] <- sum(d2/(d2+lambda.v))
}
return(df.v)
}
beta_hat <- function(x,y,lambda) {
#function that computes for a given data and lambda the value
#of the vecotr of regressors and the matrices H and Hlam
p <- dim(x)[2]
H.lam <- solve(t(x)%*%x + lambda*diag(1,p))%*%t(x)
beta.hat <- H.lam%*%y
Hhat <- x%*%H.lam
values <- list(H=H.lam,beta=beta.hat,Hlam=Hhat)
}
fit.ridge <- function(x,y,xval,yval,lambda) {
#function that computes for a given data and lambda
#the model that best fits the data and returns the
#mpse and the vector of betas
p <- dim(x)[2]
reg <- beta_hat(x,y,lambda)
y.hat <- xval%*%reg$beta
mspe <- (1/length(yval))*t((yval-y.hat))%*%(yval-y.hat)
values <- list(beta=reg$beta,mspe=mspe)
return(values)
}
loo.g.cv <- function(x,y,lambda,type="loocv") {
#function that computes for a given data set and vector
#of lambdas the value of the E(mspe) computed by gcv o loocv
n.lambdas <- length(lambda)
df.v <- cal.edf(x,lambda=lambda)
n <- dim(x)[1]
results <- matrix(0,nrow = n.lambdas, ncol = 2)
for (l in 1:n.lambdas){
reg <- beta_hat(x,y,lambda[l])
hat.Y <- x%*%reg$beta
nu <- sum(diag(reg$Hlam))
if (type=="gcv") {a<-nu/n} else {a<-diag(reg$Hlam)}
results[l,] <- c(lambda.v[l],sum(((y-hat.Y)/(1-a))^2 )/n)
}
plot.lam(results=results,lambda=lambda.v,df=df.v)
opt <- c(results[which.min(results[,2]),],df.v[which.min(results[,2])])
return(opt)
}
library(MASS)
library(caret)
data("Boston")
set.seed(5647)
#we set a seed to generate pseudo random numbers
ridgereg <- function(x,y,xval,yval,lambda,plot=TRUE) {
p <- dim(x)[2] #number of parameters
n <- dim(x)[1] #number of observations
#empty results matrix
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (i in 1:length(lambda)) {
mspe <- fit.ridge(x,y,xval,yval,lambda=lambda[i])$mspe
results[i,] <- c(lambda[i],mspe)
}
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if(plot == TRUE){
plot.lam(results=results,lambda=lambda,df=df)
}
return(results)
}
ridgereg.cv <- function(x,y,k,lambda,plot=TRUE) {
p <- dim(x)[2]
n <- dim(x)[1]
flds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
kresults <- matrix(0,nrow =length(lambda) , ncol = k)
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (j in 1:length(flds)) {
filt <- unlist(flds[j], use.names=FALSE)
mpse <- ridgereg(x=x[-filt,],y=y[-filt,],
xval=x[filt,],yval=y[filt,],
lambda = lambda,
plot = FALSE)
kresults[,j] <- mpse[,2]
}
results[,1] <- lambda
results[,2] <- rowMeans(kresults)
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if (plot==TRUE) {
plot.lam(results=results,lambda=lambda,df=df)
}
value <- results[which.min(results[,2]),]
return(value)
}
prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)
train.index <- which(prostate$train==TRUE)
test <- prostate[-train.index,]
train <- prostate[train.index,]
#Data for point 3.1
Y <- scale( train$lpsa, center=TRUE, scale=FALSE)
X <- scale( as.matrix(train[,1:8]), center=TRUE, scale=TRUE)
Yval <- scale( test$lpsa, center=TRUE, scale=FALSE)
Xval <- scale( as.matrix(test[,1:8]), center=TRUE, scale=TRUE)
#Data for points 3.2, 3.2, 3.3, 3.4
Xtot <- scale(as.matrix(prostate[,1:8]),center=TRUE , scale = TRUE)
Ytot <- scale(prostate$lpsa, center=TRUE , scale=FALSE)
#We create a matrix to store the optimal lambdas
optim.lambdas <- matrix(0,ncol=4,nrow=5)
colnames(optim.lambdas) <- c("method","lambda","pmse","df")
optim.lambdas <- data.frame(optim.lambdas)
#We generate a sequence of lambdas
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
results.mspe <- ridgereg(x = X, y = Y, xval = Xval , yval = Yval ,lambda = lambda.v,plot = TRUE)
value <- results.mspe[which.min(results.mspe[,2]),]
optim.lambdas[1,] <- c("mspe",round(value,4))
results.cv5 <- ridgereg.cv(x=Xtot,y=Ytot,k=5,lambda=lambda.v,plot=TRUE)
optim.lambdas[2,] <- c("5 fold cv",round(results.cv5,4))
results.cv10 <- ridgereg.cv(x=Xtot,y=Ytot,k=10,lambda=lambda.v,plot=TRUE)
optim.lambdas[3,] <- c("10 fold cv",round(results.cv10,4))
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="loocv")
optim.lambdas[4,] <- c("loo-cv",round(result.loo,4))
result.loo <- loo.g.cv(x=Xtot,y=Ytot,lambda=lambda.v,type="gcv")
optim.lambdas[5,] <- c("gcv",round(result.loo,4))
optim.lambdas
data(Boston)
Y <- scale( Boston$medv, center=TRUE, scale=FALSE)
X <- scale( as.matrix(Boston[,-c(14)]), center=TRUE, scale=TRUE)
p <- dim(X)[2]
n <- dim(X)[1]
df<-data.frame(X,Y)
optim.lambda <- matrix(0,ncol=4,nrow=3)
colnames(optim.lambda) <- c("method","lambda","pmse","df")
optim.lambda <- data.frame(optim.lambda)
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
# k- fold 5 cross-validation
results.cv5 <- ridgereg.cv(x=X,y=Y,k=5,lambda=lambda.v,plot=TRUE)
optim.lambda[1,] <- c("5 fold cv",round(results.cv5,4))
#k - fold 10 cross-validation
results.cv10 <- ridgereg.cv(x=X,y=Y,k=10,lambda=lambda.v,plot=TRUE)
optim.lambda[2,] <- c("10 fold cv",round(results.cv10,4))
#loo
results.loo <- loo.g.cv(x=X,y=Y,lambda=lambda.v,type="loocv")
optim.lambda[3,] <- c("loo",round(results.loo,4))
optim.lambda
model <- fit.ridge(x=X,y=Y,xval=X,yval=Y,lambda=as.numeric(optim.lambda[3,2]))
model$mspe
model$beta
optim.lambdas
knitr::opts_chunk$set(echo = TRUE)
optim.lambdas
optim.lambdas
optim.lambdas
optim.lambdas
