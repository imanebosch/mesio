kresults <- matrix(0,nrow =length(lambda) , ncol = k)
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (j in 1:length(flds)) {
filt <- unlist(flds[j], use.names=FALSE)
mpse <- ridgereg(x=x[-filt,],y=y[-filt,],
xval=x[filt,],yval=y[filt,],
lambda = lambda,
plot = FALSE)
kresults[,j] <- mpse[,2]
}
results[,1] <- lambda
results[,2] <- rowMeans(kresults)
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if (plot==TRUE) {
plot.lam(results=results,lambda=lambda,df=df)
}
#value <- results[which.min(results[,2]),]
value <- results
return(value)
}
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
n <- dim(Y.scale)[1]
sd_y <- sqrt(var(Y.scale)*(n-1)/n)[1,1]
t.lambda <- lambda.v*sd_y/n
CV.CMEDV.ridge <- cv.glmnet(x = X.scale, y = Y.scale, alpha = 0,
lambda = t.lambda, nfolds= 10,standardize = FALSE,
intercept = FALSE,thresh = 1e-20, type.measure = "mse")
lambda.opt.ridge <- CV.CMEDV.ridge$lambda.min
lambda.opt.ridge.1se <- CV.CMEDV.ridge$lambda.1se
#Lasso regression of the variable CMEDV
CMEDV.ridge <- glmnet(X.scale,Y.scale, standardize = FALSE ,
intercept = FALSE, alpha = 0,lambda = t.lambda)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.ridge, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.ridge, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.ridge$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.ridge$lambda.1se),col=2,lty=2)
df.lambda <- cal.edf(x = X.scale, lambda = lambda.v)
df.lambda.min <- cal.edf(x = X.scale, lambda = lambda.opt.ridge*n/sd_y)
df.lambda.1se <- cal.edf(x = X.scale, lambda = lambda.opt.ridge.1se*n/sd_y)
mse <- CV.CMEDV.ridge$cvm
plot(df.lambda,rev(mse),
xlab="df(lambda)",
ylab = "mse",
main="MSPE by df(lambda)",
cex.main=0.9, cex.axis=0.8)
abline(v=df.lambda.min,col=2,lty=2)
abline(v=df.lambda.1se,col=2,lty=2)
#coefficient of the optimal lambda
coef.lambda.min <- coef(CMEDV.ridge, s = CV.CMEDV.ridge$lambda.min)
#coefficient of the optimal lambda 1se
coef.lambda.1se <- coef(CMEDV.ridge, s = CV.CMEDV.ridge$lambda.1se)
#Total
Fin.bet.lamb <- as.matrix(cbind(coef.lambda.min,coef.lambda.1se))
colnames(Fin.bet.lamb) <- c("lambda.min","lambda.1se")
#Print Results
print("Optimal lambdas glmnet scale")
c(CV.CMEDV.ridge$lambda.min, CV.CMEDV.ridge$lambda.1se)
print("Optimal lambda coefficients for the Ridge Regression")
print(round(Fin.bet.lamb,3))
CV.CMEDV.ridge.function <- ridgereg.cv(x=X.scale,y=Y.scale,k=10,lambda=lambda.v,plot=TRUE)
lambda.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),1]
df.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),3]
CMEDV.ridge.function.coef <- fit.ridge(x=X.scale,y=Y.scale,xval=X.scale,yval=Y.scale,lambda=lambda.opt.ridge.func)
print("Optimal lambda")
lambda.opt.ridge.func
print("Coefficients of the optimal lambda")
CMEDV.ridge.function.coef$beta
lambda.opt.ridge.func
lambda.opt.ridge.func
lambda.opt.ridge.func
df.opt.ridge.func
lambda.opt.ridge.func
CV.CMEDV.ridge.function <- ridgereg.cv(x=X.scale,y=Y.scale,k=10,lambda=lambda.v,plot=TRUE)
lambda.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),1]
df.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),3]
CMEDV.ridge.function.coef <- fit.ridge(x=X.scale,y=Y.scale,xval=X.scale,yval=Y.scale,lambda=lambda.opt.ridge.func)
print("Optimal lambda")
lambda.opt.ridge.func
print("Coefficients of the optimal lambda")
CMEDV.ridge.function.coef$beta
CV.CMEDV.ridge.function <- ridgereg.cv(x=X.scale,y=Y.scale,k=10,lambda=lambda.v,plot=TRUE)
lambda.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),1]
df.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),3]
CMEDV.ridge.function.coef <- fit.ridge(x=X.scale,y=Y.scale,xval=X.scale,yval=Y.scale,lambda=lambda.opt.ridge.func)
print("Optimal lambda")
lambda.opt.ridge.func
print("Coefficients of the optimal lambda")
CMEDV.ridge.function.coef$beta
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.lasso, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.lasso, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.lasso$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.lasso$lambda.1se),col=2,lty=2)
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.lasso, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.lasso, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.lasso$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.lasso$lambda.1se),col=2,lty=2)
#coefficient of the optimal lambda
coef.lambda.min <- coef(CMEDV.lasso, s = CV.CMEDV.lasso$lambda.min)
#coefficient of the optimal lambda 1se
coef.lambda.1se <- coef(CMEDV.lasso, s = CV.CMEDV.lasso$lambda.1se)
#Total
Fin.lamb <- as.matrix(cbind(coef.lambda.min,coef.lambda.1se))
colnames(Fin.lamb) <- c("beta.lambda.min","beta.lambda.1se")
print("Optimal lambda coefficients for the Lasso Regression")
round(Fin.lamb,3)
library(glmnet)
library(caret)
set.seed(3456)
load("boston.Rdata")
boston <- boston.c
remove(boston.c, boston.soi, boston.utm)
#Import de boston corrected dataset to the boston data frame
#Non scaled data
Y <- boston$CMEDV
X <- apply(as.matrix(boston[,8:20]),2,as.numeric)
#Scaled data
Y.scale <- scale(Y,center=TRUE,scale = FALSE)
X.scale <- scale(X,center=TRUE,scale = TRUE)
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.lasso, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.lasso, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.lasso$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.lasso$lambda.1se),col=2,lty=2)
#coefficient of the optimal lambda
coef.lambda.min <- coef(CMEDV.lasso, s = CV.CMEDV.lasso$lambda.min)
#coefficient of the optimal lambda 1se
coef.lambda.1se <- coef(CMEDV.lasso, s = CV.CMEDV.lasso$lambda.1se)
#Total
Fin.lamb <- as.matrix(cbind(coef.lambda.min,coef.lambda.1se))
colnames(Fin.lamb) <- c("beta.lambda.min","beta.lambda.1se")
#Print Results
#optimal lambda for lasso regression with 10-fold cros-validation test
print("Optimal lambda for the Lasso Regression")
lambda.opt.lasso
print("Optimal lambda coefficients for the Lasso Regression")
round(Fin.lamb,3)
CMEDV.predic <- predict(CMEDV.lasso, s = lambda.opt.lasso, newx = X.scale)
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(x=CMEDV.predic, y=Y.scale, pch=18, cex=.5,
cex.lab=0.9, cex.axis=0.8, cex.main=0.9,
xlab = "Predicted CMEDV values",
ylab =  "Original CMEDV values",
main = "Predicted vs. Original" )
abline(a=0,b=1,col=2,lty=1)
error <- CMEDV.predic - Y.scale
hist(x = error, breaks = 16,freq = FALSE,
main = "Residuals histogram",
cex.lab=0.9, cex.axis=0.8, cex.main=0.9)
x.norm <- seq(-30,30,0.1)
y.norm <- dnorm(x = x.norm , mean = mean(error), sd = sd(error))
lines(x = x.norm,y=y.norm,col=3,lty=2)
abline(v=mean(error),col=2,lty=2)
##################################
##R FUNCTIONS FOR THE 10-FOLD CROSS VALIDATION TEST##
##################################
#Code with the definition of the functions used throughout the document for the k-fold cross valitadion test. This piece of code won't be shown in the pdf document
plot.lam <- function(results,lambda,df) {
#function that plots the mspe agains both lambda and df(lambda)
par(mfrow=c(1,2),oma = c(2,3,1,0), mar = c(4,2,2,1), mgp=c(1.5,0.5,0))
plot(log(1+results[,1]),results[,2],
xlab="log(1+lambda)",
ylab = "",
main="MSPE by lambda values",
cex.main=0.9, cex.axis=0.8)
lam.opt <- results[results[,2]==min(results[,2]),1]
abline(v=log(1+lam.opt),col=2,lty=2)
plot(df,results[,2],
xlab="df(lambda)",
ylab = "",
main="MSPE by df(lambda)",
cex.main=0.9,cex.axis=0.8)
lam.opt <- df[results[,2]==min(results[,2])]
abline(v=lam.opt,col=2,lty=2)
mtext(text="MSPE",side=2,line=0,outer=TRUE)
}
cal.edf <- function(x,lambda) {
#function that computes the efective degrees of freedom for a
#given vecotor of lambdas and a data set x
n.lam <- length(lambda)
d2 <- eigen(t(x)%*%x ,symmetric = TRUE, only.values = TRUE)$values
p <- dim(x)[2]
n <- dim(x)[1]
df.v <- NULL
for (i in 1:n.lam){
lambda.v <- lambda[i]
df.v[i] <- sum(d2/(d2+lambda.v))
}
return(df.v)
}
beta_hat <- function(x,y,lambda) {
#function that computes for a given data and lambda the value
#of the vecotr of regressors and the matrices H and Hlam
p <- dim(x)[2]
H.lam <- solve(t(x)%*%x + lambda*diag(1,p))%*%t(x)
beta.hat <- H.lam%*%y
Hhat <- x%*%H.lam
values <- list(H=H.lam,beta=beta.hat,Hlam=Hhat)
}
fit.ridge <- function(x,y,xval,yval,lambda) {
#function that computes for a given data and lambda
#the model that best fits the data and returns the
#mpse and the vector of betas
p <- dim(x)[2]
reg <- beta_hat(x,y,lambda)
y.hat <- xval%*%reg$beta
mspe <- (1/length(yval))*t((yval-y.hat))%*%(yval-y.hat)
values <- list(beta=reg$beta,mspe=mspe)
return(values)
}
ridgereg <- function(x,y,xval,yval,lambda,plot=TRUE) {
p <- dim(x)[2] #number of parameters
n <- dim(x)[1] #number of observations
#empty results matrix
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (i in 1:length(lambda)) {
mspe <- fit.ridge(x,y,xval,yval,lambda=lambda[i])$mspe
results[i,] <- c(lambda[i],mspe)
}
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if(plot == TRUE){
plot.lam(results=results,lambda=lambda,df=df)
}
return(results)
}
ridgereg.cv <- function(x,y,k,lambda,plot=TRUE) {
p <- dim(x)[2]
n <- dim(x)[1]
flds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
kresults <- matrix(0,nrow =length(lambda) , ncol = k)
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (j in 1:length(flds)) {
filt <- unlist(flds[j], use.names=FALSE)
mpse <- ridgereg(x=x[-filt,],y=y[-filt,],
xval=x[filt,],yval=y[filt,],
lambda = lambda,
plot = FALSE)
kresults[,j] <- mpse[,2]
}
results[,1] <- lambda
results[,2] <- rowMeans(kresults)
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if (plot==TRUE) {
plot.lam(results=results,lambda=lambda,df=df)
}
#value <- results[which.min(results[,2]),]
value <- results
return(value)
}
lambda.max <- 1e5
n.lambdas <- 50
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
n <- dim(Y.scale)[1]
sd_y <- sqrt(var(Y.scale)*(n-1)/n)[1,1]
t.lambda <- lambda.v*sd_y/n
CV.CMEDV.ridge <- cv.glmnet(x = X.scale, y = Y.scale, alpha = 0,
lambda = t.lambda, nfolds= 10,standardize = FALSE,
intercept = FALSE,thresh = 1e-20, type.measure = "mse")
lambda.opt.ridge <- CV.CMEDV.ridge$lambda.min
lambda.opt.ridge.1se <- CV.CMEDV.ridge$lambda.1se
#Lasso regression of the variable CMEDV
CMEDV.ridge <- glmnet(X.scale,Y.scale, standardize = FALSE ,
intercept = FALSE, alpha = 0,lambda = t.lambda)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.ridge, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.ridge, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.ridge$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.ridge$lambda.1se),col=2,lty=2)
df.lambda <- cal.edf(x = X.scale, lambda = lambda.v)
df.lambda.min <- cal.edf(x = X.scale, lambda = lambda.opt.ridge*n/sd_y)
df.lambda.1se <- cal.edf(x = X.scale, lambda = lambda.opt.ridge.1se*n/sd_y)
mse <- CV.CMEDV.ridge$cvm
plot(df.lambda,rev(mse),
xlab="df(lambda)",
ylab = "mse",
main="MSPE by df(lambda)",
cex.main=0.9, cex.axis=0.8)
abline(v=df.lambda.min,col=2,lty=2)
abline(v=df.lambda.1se,col=2,lty=2)
#coefficient of the optimal lambda
coef.lambda.min <- coef(CMEDV.ridge, s = CV.CMEDV.ridge$lambda.min)
#coefficient of the optimal lambda 1se
coef.lambda.1se <- coef(CMEDV.ridge, s = CV.CMEDV.ridge$lambda.1se)
#Total
Fin.bet.lamb <- as.matrix(cbind(coef.lambda.min,coef.lambda.1se))
colnames(Fin.bet.lamb) <- c("lambda.min","lambda.1se")
#Print Results
print("Optimal lambdas glmnet scale")
c(CV.CMEDV.ridge$lambda.min, CV.CMEDV.ridge$lambda.1se)
print("Optimal lambda coefficients for the Ridge Regression")
print(round(Fin.bet.lamb,3))
CV.CMEDV.ridge.function <- ridgereg.cv(x=X.scale,y=Y.scale,k=10,lambda=lambda.v,plot=TRUE)
lambda.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),1]
df.opt.ridge.func <- CV.CMEDV.ridge.function[which.min(CV.CMEDV.ridge.function[,2]),3]
CMEDV.ridge.function.coef <- fit.ridge(x=X.scale,y=Y.scale,xval=X.scale,yval=Y.scale,lambda=lambda.opt.ridge.func)
print("Optimal lambda")
lambda.opt.ridge.func
print("Coefficients of the optimal lambda")
CMEDV.ridge.function.coef$beta
lambdas.fin <- c(CV.CMEDV.ridge$lambda.min*n/sd_y,lambda.opt.ridge.func)
names(lambdas.fin) <- c("Glmnet lambda","Theory lambda")
print(lambdas.fin)
CV.CMEDV.ridge$lambda.min
lambdas.fin <- c(CV.CMEDV.ridge$lambda.min*n/sd_y,lambda.opt.ridge.func)
names(lambdas.fin) <- c("Glmnet lambda","Theory lambda")
print(lambdas.fin)
Final.betas <- cbind(matrix(Fin.bet.lamb[-1,1], ncol = 1),CMEDV.ridge.function.coef$beta)
colnames(Final.betas) <- c("beta Glmnet lambda","beta Theory lambda")
print(Final.betas)
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.lasso, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.lasso, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.lasso$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.lasso$lambda.1se),col=2,lty=2)
main("AFaf")
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.lasso, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.lasso, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.lasso$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.lasso$lambda.1se),col=2,lty=2)
title("AFaf")
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
tite("AFaf")
#10-fold cross validation test in Lasso Regression
k <- 10
CV.CMEDV.lasso <- cv.glmnet(x = X.scale, y = Y.scale,
standardize=FALSE, intercept=FALSE, nfolds=k)
lambda.opt.lasso <- CV.CMEDV.lasso$lambda.min
#Lasso regression of the variable CMEDV
CMEDV.lasso <- glmnet(X.scale,Y.scale, standardize = FALSE , intercept = FALSE)
#plot results Lasso regression
par(mfrow=c(1,2), mgp=c(1.5,0.5,0), oma = c(3,0,1,0), mar = c(4,3,2,1))
plot(CV.CMEDV.lasso, ylab="Mean Square Error",
cex.axis=0.8, cex.lab=0.9)
plot(CMEDV.lasso, xvar="lambda",
cex.axis=0.8, cex.lab=0.9)
abline(v=log(CV.CMEDV.lasso$lambda.min),col=2,lty=2)
abline(v=log(CV.CMEDV.lasso$lambda.1se),col=2,lty=2)
##################################
##R FUNCTIONS FOR THE 10-FOLD CROSS VALIDATION TEST##
##################################
#Code with the definition of the functions used throughout the document for the k-fold cross valitadion test. This piece of code won't be shown in the pdf document
plot.lam <- function(results,lambda,df) {
#function that plots the mspe agains both lambda and df(lambda)
par(mfrow=c(1,2),oma = c(2,3,1,0), mar = c(4,2,2,1), mgp=c(1.5,0.5,0))
plot(log(1+results[,1]),results[,2],
xlab="log(1+lambda)",
ylab = "",
main="MSPE by lambda values",
cex.main=0.9, cex.axis=0.8)
lam.opt <- results[results[,2]==min(results[,2]),1]
abline(v=log(1+lam.opt),col=2,lty=2)
plot(df,results[,2],
xlab="df(lambda)",
ylab = "",
main="MSPE by df(lambda)",
cex.main=0.9,cex.axis=0.8)
lam.opt <- df[results[,2]==min(results[,2])]
abline(v=lam.opt,col=2,lty=2)
mtext(text="MSPE",side=2,line=0,outer=TRUE)
}
cal.edf <- function(x,lambda) {
#function that computes the efective degrees of freedom for a
#given vecotor of lambdas and a data set x
n.lam <- length(lambda)
d2 <- eigen(t(x)%*%x ,symmetric = TRUE, only.values = TRUE)$values
p <- dim(x)[2]
n <- dim(x)[1]
df.v <- NULL
for (i in 1:n.lam){
lambda.v <- lambda[i]
df.v[i] <- sum(d2/(d2+lambda.v))
}
return(df.v)
}
beta_hat <- function(x,y,lambda) {
#function that computes for a given data and lambda the value
#of the vecotr of regressors and the matrices H and Hlam
p <- dim(x)[2]
H.lam <- solve(t(x)%*%x + lambda*diag(1,p))%*%t(x)
beta.hat <- H.lam%*%y
Hhat <- x%*%H.lam
values <- list(H=H.lam,beta=beta.hat,Hlam=Hhat)
}
fit.ridge <- function(x,y,xval,yval,lambda) {
#function that computes for a given data and lambda
#the model that best fits the data and returns the
#mpse and the vector of betas
p <- dim(x)[2]
reg <- beta_hat(x,y,lambda)
y.hat <- xval%*%reg$beta
mspe <- (1/length(yval))*t((yval-y.hat))%*%(yval-y.hat)
values <- list(beta=reg$beta,mspe=mspe)
return(values)
}
ridgereg <- function(x,y,xval,yval,lambda,plot=TRUE) {
p <- dim(x)[2] #number of parameters
n <- dim(x)[1] #number of observations
#empty results matrix
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (i in 1:length(lambda)) {
mspe <- fit.ridge(x,y,xval,yval,lambda=lambda[i])$mspe
results[i,] <- c(lambda[i],mspe)
}
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if(plot == TRUE){
plot.lam(results=results,lambda=lambda,df=df)
}
return(results)
}
ridgereg.cv <- function(x,y,k,lambda,plot=TRUE) {
p <- dim(x)[2]
n <- dim(x)[1]
flds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
kresults <- matrix(0,nrow =length(lambda) , ncol = k)
results <- matrix(0,nrow = length(lambda), ncol = 2)
colnames(results) <- c("lambda","pmse")
for (j in 1:length(flds)) {
filt <- unlist(flds[j], use.names=FALSE)
mpse <- ridgereg(x=x[-filt,],y=y[-filt,],
xval=x[filt,],yval=y[filt,],
lambda = lambda,
plot = FALSE)
kresults[,j] <- mpse[,2]
}
results[,1] <- lambda
results[,2] <- rowMeans(kresults)
df <- cal.edf(x=x,lambda=lambda)
results <- cbind(results,df)
if (plot==TRUE) {
plot.lam(results=results,lambda=lambda,df=df)
}
#value <- results[which.min(results[,2]),]
value <- results
return(value)
}
